{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "importamos librerias"
      ],
      "metadata": {
        "id": "akkyD-baKMRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "jU5tG0ytJe8q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "impotamos liberias e KERAS\n"
      ],
      "metadata": {
        "id": "hcgt3cEKKbZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q --user --upgrade tfp-nightly\n",
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "0PYwiNcOKwGj"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-layer-normalization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8Z6CoL5MU5z",
        "outputId": "1c3f75d6-c6be-42a6-bc70-d0a2e9f111da"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-layer-normalization in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-layer-normalization) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential,Model\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "from keras.layers import LeakyReLU"
      ],
      "metadata": {
        "id": "HGZ85IYvKejk"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirname = os.path.join(os.getcwd(), 'ninhosimages')\n",
        "\n",
        "imgpath = dirname + os.sep\n",
        "\n",
        "\n",
        "\n",
        "images = []\n",
        "\n",
        "directories = []\n",
        "\n",
        "dircount = []\n",
        "\n",
        "prevRoot=''\n",
        "\n",
        "cant=0\n",
        "\n",
        "\n",
        "\n",
        "for root, dirnames, filenames in os.walk(imgpath):\n",
        "\n",
        "    for filename in filenames:\n",
        "\n",
        "        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
        "\n",
        "            cant=cant+1\n",
        "\n",
        "            filepath = os.path.join(root, filename)\n",
        "\n",
        "            image = plt.imread(filepath)\n",
        "\n",
        "            images.append(image)\n",
        "\n",
        "            b = \"Leyendo...\" + str(cant)\n",
        "\n",
        "            if prevRoot !=root:\n",
        "\n",
        "                prevRoot=root\n",
        "\n",
        "                directories.append(root)\n",
        "\n",
        "                dircount.append(cant)\n",
        "\n",
        "                cant=0\n",
        "\n",
        "dircount.append(cant)\n",
        "\n",
        "\n",
        "\n",
        "dircount = dircount[1:]\n",
        "\n",
        "dircount[0]=dircount[0]+1\n",
        "\n",
        "print(\"Directorios leidos: \",len(directories))\n",
        "\n",
        "print(\"Imagenes en cada directorio: \", dircount)\n",
        "\n",
        "print(\"Total de imagenes en subdirectorio: \",sum(dircount))"
      ],
      "metadata": {
        "id": "NFU4uXL0KvLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Número de muestras en X:\", len(X))\n",
        "print(\"Número de muestras en y:\", len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDfN9gMiWn21",
        "outputId": "1727245b-4212-47b1-ae62-8ff901692ee6"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de muestras en X: 1\n",
            "Número de muestras en y: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels=[]\n",
        "\n",
        "indice=0\n",
        "\n",
        "for cantidad in dircount:\n",
        "\n",
        "    for i in range(cantidad):\n",
        "\n",
        "        labels.append(indice)\n",
        "\n",
        "    indice=indice+1\n",
        "\n",
        "print(\"Cantidad etiquetas creadas: \",len(labels))\n",
        "\n",
        "\n",
        "\n",
        "ninhos=[]\n",
        "\n",
        "indice=0\n",
        "\n",
        "for directorio in directories:\n",
        "\n",
        "    name = directorio.split(os.sep)\n",
        "\n",
        "    print(indice , name[len(name)-1])\n",
        "\n",
        "    ninhos.append(name[len(name)-1])\n",
        "\n",
        "    indice=indice+1\n",
        "\n",
        "\n",
        "\n",
        "y = np.array(labels)\n",
        "\n",
        "X = np.array(images, dtype=np.uint8) #convierto de lista a numpy uint8\n",
        "\n",
        "\n",
        "\n",
        "# Buscar los números únicos de las etiquetas del aprendizaje\n",
        "\n",
        "classes = np.unique(y)\n",
        "\n",
        "nClasses = len(classes)\n",
        "\n",
        "print('Total de outputs : ', nClasses)\n",
        "\n",
        "print('Output classes : ', classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50fiqEajNvFM",
        "outputId": "e1d6a378-b0bc-41d0-d227-b9bbdcfd92b4"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad etiquetas creadas:  1\n",
            "0 \n",
            "Total de outputs :  1\n",
            "Output classes :  [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mezclar todo y crear los grupos de entrenamiento y testing\n",
        "\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "print('Datos de aprendizaje : ', train_X.shape, train_Y.shape)\n",
        "\n",
        "print('Testing de aprendizaje : ', test_X.shape, test_Y.shape)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=[5,5])\n",
        "\n",
        "\n",
        "\n",
        "# Display the first image in training data\n",
        "\n",
        "plt.subplot(121)\n",
        "\n",
        "plt.imshow(train_X[0,:,:], cmap='gray')\n",
        "\n",
        "plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
        "\n",
        "\n",
        "\n",
        "# Display the first image in testing data\n",
        "\n",
        "plt.subplot(122)\n",
        "\n",
        "plt.imshow(test_X[0,:,:], cmap='gray')\n",
        "\n",
        "plt.title(\"Ground Truth : {}\".format(test_Y[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "TfX3rY_fN0B6",
        "outputId": "089c4ccb-c50e-4e14-e539-3ac1b455f097"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-9cc26d1508ec>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = train_X.astype('float32')\n",
        "\n",
        "test_X = test_X.astype('float32')\n",
        "\n",
        "train_X = train_X / 255.\n",
        "\n",
        "test_X = test_X / 255.\n",
        "\n",
        "\n",
        "\n",
        "# Change the labels from categorical to one-hot encoding\n",
        "\n",
        "train_Y_one_hot = to_categorical(train_Y)\n",
        "\n",
        "test_Y_one_hot = to_categorical(test_Y)\n",
        "\n",
        "\n",
        "\n",
        "# Display the change for category label using one-hot encoding\n",
        "\n",
        "print('Etiqueta original:', train_Y[0])\n",
        "\n",
        "print('Después de la conversión a one-hot:', train_Y_one_hot[0])"
      ],
      "metadata": {
        "id": "ZbFyNC__N2q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mezclar todo y crear los grupos de entrenamiento y testing\n",
        "\n",
        "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)\n",
        "\n",
        "\n",
        "\n",
        "print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)"
      ],
      "metadata": {
        "id": "XHifUaoTN6j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declaramos variables con los parámetros de configuración de la red\n",
        "\n",
        "INIT_LR = 1e-3\n",
        "\n",
        "epochs = 6\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "\n",
        "ninhos_model = Sequential()\n",
        "\n",
        "ninhos_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(21,28,3)))\n",
        "\n",
        "ninhos_model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "ninhos_model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "\n",
        "ninhos_model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "ninhos_model.add(Flatten())\n",
        "\n",
        "ninhos_model.add(Dense(32, activation='linear'))\n",
        "\n",
        "ninhos_model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "ninhos_model.add(Dropout(0.5))\n",
        "\n",
        "ninhos_model.add(Dense(nClasses, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "ninhos_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "ninhos_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KfXEl1zJOJup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar entrenamiento\n",
        "\n",
        "# Este paso puede tomar varios minutos, dependiendo del cpu y memoria ram\n",
        "\n",
        "ninhos_train = ninhos_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
        "\n",
        "\n",
        "\n",
        "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
        "\n",
        "ninhos_model.save(\"ninhos_mnist.h5py\")"
      ],
      "metadata": {
        "id": "WsatuwFjOMlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eval = ninhos_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "print('TEST:')\n",
        "\n",
        "print('Perdida:', test_eval[0])\n",
        "\n",
        "print('Exactitud:', test_eval[1])\n",
        "\n",
        "\n",
        "\n",
        "accuracy = ninhos_train.history['accuracy']\n",
        "\n",
        "val_accuracy = ninhos_train.history['val_accuracy']\n",
        "\n",
        "loss = ninhos_train.history['loss']\n",
        "\n",
        "val_loss = ninhos_train.history['val_loss']\n",
        "\n",
        "epochs = range(len(accuracy))\n",
        "\n",
        "plt.plot(epochs, accuracy, 'bo', label='Precision de entrenamiento')\n",
        "\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Exactitud de la validacion')\n",
        "\n",
        "plt.title('Precisión de entrenamiento y validación')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Perdida de entrenamiento')\n",
        "\n",
        "plt.plot(epochs, val_loss, 'b', label='Perdida de validacion')\n",
        "\n",
        "plt.title('Perdida de entrenamiento y validacion')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_classes2 = ninhos_model.predict(test_X)\n",
        "\n",
        "\n",
        "\n",
        "predicted_classes=[]\n",
        "\n",
        "for predicted_ninhos in predicted_classes2:\n",
        "\n",
        "    predicted_classes.append(predicted_ninhos.tolist().index(max(predicted_ninhos)))\n",
        "\n",
        "predicted_classes=np.array(predicted_classes)\n",
        "\n",
        "\n",
        "\n",
        "predicted_classes.shape, test_Y.shape"
      ],
      "metadata": {
        "id": "srVhqAJrOOqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}